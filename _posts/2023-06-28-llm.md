---
layout: post
title: "My take on LLMs replacing programmers"
comments_id: 35
---

*Disclaimer: This is not to affirm or deny anything. I just cast my thoughts, doubts, and suspicions, in some cases, I may be wrong, to generate a thoughtful discussion.*

Large language models (LLMs) are sophisticated probability functions. They take a prompt and then generate a probability distribution of known classes (words in the case of LLMs). Having seen sufficient examples of a kind, it will know what words are most likely to occur next. It doesn't have a sense of right or wrong. It doesn't know if the generated response is right or wrong. It gives you the most probable output that occurs in reality. So, it will be biased towards the majority. Is the majority always right? Should we trust the majority blindly? Sure, it can generate a second most popular or third most popular opinion, which needs an appropriate prompt from the user. So, the question is, can LLMs ever replace programmers?

- **Problem 1 - Proprietary data:**
The most obvious problem is the data. Deep learning models are not sentient, as I noted before. They develop their predictability by learning the patterns from tons of data. LLMs can replace any repetitive jobs, not just programming, as long as a huge amount of training data is available. However, in the real world, there is much-specialized software; in fact, most reputed companies develop their specialized product. For example, we used to interface different biometric hardware devices with software at TigerIT Bangladesh. The device specification wildly varied across vendors. We had to take onsite training in Hong Kong for a Hong-kong-based product. This is just an example. Data is the business. So, who would expose their proprietary data and business secrets for some LLMs, making them go out of business themselves? So, the data is a real bottleneck here.

- **Problem 2 - Under-developed areas:**
Again, it's a data-related one. A data-driven model can successfully learn a repetitive, widely available domain. Understandably, LLMs would struggle in the areas that are seldom explored. Humans are still needed to flourish and advance the area to the extent it becomes ubiquitous.

- **Problem 3 - Thought gaps:**
What LLMs see are end-products of human thoughts because only end-products are documented. When a novel system is developed, we don't just come up with the end product. It follows a step-by-step reasoning and validation process. Such thought gaps would be lost in the machine, at least in its current state. Furthermore, as I said before, LLMs don't have a sense of right or wrong, and it would not be capable of validating intermediate steps. Would LLMs be able to draw an analogy or connect two seemingly unrelated concepts- an important trait of the human thought process? So, when developing a new system, the undocumented human thought process would be missing, and LLMs would be limited in this context.

Is anything else on your mind?
